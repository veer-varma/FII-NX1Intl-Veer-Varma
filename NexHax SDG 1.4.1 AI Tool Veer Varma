{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veer-varma/FII-NX1Intl-Veer-Varma/blob/main/NexHax%20SDG%201.4.1%20AI%20Tool%20Veer%20Varma\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf5KrEb6vrkR"
      },
      "source": [
        "# Import Libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import plotly.express as px\n",
        "from functools import lru_cache\n",
        "import gradio as gr\n"
      ],
      "metadata": {
        "id": "cipGopNHfSwC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJBs_flRovLc"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gJr_9dXGpJ05",
        "outputId": "75f03395-5a55-4409-d6de-803ee8865c69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4ef7857e-3889-4221-b963-8d76a1a84e6b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4ef7857e-3889-4221-b963-8d76a1a84e6b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Dataset.ods to Dataset.ods\n",
            "Collecting odfpy\n",
            "  Downloading odfpy-1.4.1.tar.gz (717 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.0/717.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from odfpy) (0.7.1)\n",
            "Building wheels for collected packages: odfpy\n",
            "  Building wheel for odfpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for odfpy: filename=odfpy-1.4.1-py2.py3-none-any.whl size=160673 sha256=3eb6c89c6a1de117646c86b966bbfe3256dbaaf5a536bdfe82b4269be1488933\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/1d/c8/8c29be1d73ca42d15977c75193d9f39a98499413c2838ac54c\n",
            "Successfully built odfpy\n",
            "Installing collected packages: odfpy\n",
            "Successfully installed odfpy-1.4.1\n",
            "Dataset preview:\n",
            "   Goal  Target Indicator      SeriesCode  \\\n",
            "0     1     1.4     1.4.1  SP_ACS_BSRVH2O   \n",
            "1     1     1.4     1.4.1  SP_ACS_BSRVH2O   \n",
            "2     1     1.4     1.4.1  SP_ACS_BSRVH2O   \n",
            "3     1     1.4     1.4.1  SP_ACS_BSRVH2O   \n",
            "4     1     1.4     1.4.1  SP_ACS_BSRVH2O   \n",
            "\n",
            "                                   SeriesDescription  GeoAreaCode GeoAreaName  \\\n",
            "0  Proportion of population using basic drinking ...            1       World   \n",
            "1  Proportion of population using basic drinking ...            1       World   \n",
            "2  Proportion of population using basic drinking ...            1       World   \n",
            "3  Proportion of population using basic drinking ...            1       World   \n",
            "4  Proportion of population using basic drinking ...            1       World   \n",
            "\n",
            "   TimePeriod     Value  Time_Detail  ...  UpperBound  LowerBound  BasePeriod  \\\n",
            "0        2010  86.18300         2010  ...         NaN         NaN         NaN   \n",
            "1        2010  95.80767         2010  ...         NaN         NaN         NaN   \n",
            "2        2010  76.00963         2010  ...         NaN         NaN         NaN   \n",
            "3        2011  86.57294         2011  ...         NaN         NaN         NaN   \n",
            "4        2011  95.84988         2011  ...         NaN         NaN         NaN   \n",
            "\n",
            "                                              Source GeoInfoUrl  FootNote  \\\n",
            "0  WHO/UNICEF Joint Monitoring Programme for Wate...        NaN       NaN   \n",
            "1  WHO/UNICEF Joint Monitoring Programme for Wate...        NaN       NaN   \n",
            "2  WHO/UNICEF Joint Monitoring Programme for Wate...        NaN       NaN   \n",
            "3  WHO/UNICEF Joint Monitoring Programme for Wate...        NaN       NaN   \n",
            "4  WHO/UNICEF Joint Monitoring Programme for Wate...        NaN       NaN   \n",
            "\n",
            "   Location Nature Reporting Type    Units  \n",
            "0   ALLAREA      N              G  PERCENT  \n",
            "1     URBAN      N              G  PERCENT  \n",
            "2     RURAL      N              G  PERCENT  \n",
            "3   ALLAREA      N              G  PERCENT  \n",
            "4     URBAN      N              G  PERCENT  \n",
            "\n",
            "[5 rows x 21 columns]\n",
            "Sheet columns: Index(['Goal', 'Target', 'Indicator', 'SeriesCode', 'SeriesDescription',\n",
            "       'GeoAreaCode', 'GeoAreaName', 'TimePeriod', 'Value', 'Time_Detail',\n",
            "       'TimeCoverage', 'UpperBound', 'LowerBound', 'BasePeriod', 'Source',\n",
            "       'GeoInfoUrl', 'FootNote', 'Location', 'Nature', 'Reporting Type',\n",
            "       'Units'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Upload your dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # This will prompt you to select a file\n",
        "\n",
        "filename = list(uploaded.keys())[0]  # Gets the uploaded file name\n",
        "\n",
        "# For .ods files, make sure odfpy is installed\n",
        "!pip install odfpy\n",
        "\n",
        "# Read the .ods file\n",
        "df = pd.read_excel(filename, engine='odf')\n",
        "\n",
        "# Inspect the data\n",
        "print(\"Dataset preview:\")\n",
        "print(df.head())\n",
        "print(\"Sheet columns:\", df.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwuxHmxllTwN"
      },
      "source": [
        "## LSTM model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM model for time series forecasting\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=20, output_size=1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, seq_len, features]\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.linear(out[:, -1, :])  # take the last timestep\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "HcMRscMziMr_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset for time Series"
      ],
      "metadata": {
        "id": "xmevBoJNPxTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class for sliding windows\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, series, window_size=3):\n",
        "        self.series = torch.tensor(series, dtype=torch.float32).unsqueeze(-1)  # [seq, 1]\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, len(self.series) - self.window_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.series[idx:idx+self.window_size], self.series[idx+self.window_size]"
      ],
      "metadata": {
        "id": "gQA8_6cYP5V6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Rh3-Vt9Nev9"
      },
      "source": [
        "## Train LSTM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train LSTM on a series\n",
        "def train_lstm(values, window_size=3, epochs=500, lr=0.01):\n",
        "    if len(values) < window_size + 1:\n",
        "        return None  # Not enough data\n",
        "    values_norm = np.array(values) / 100.0\n",
        "    dataset = TimeSeriesDataset(values_norm, window_size)\n",
        "    loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    model = LSTM()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for seq, label in loader:\n",
        "            pred = model(seq)\n",
        "            loss = loss_fn(pred, label)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "_FwKtqWasmeB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to predict the goal year"
      ],
      "metadata": {
        "id": "mPbMYi2p0YbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict the year when value reaches 100 using the model\n",
        "@lru_cache(maxsize=None)\n",
        "def predict_year_for_series(country, location, series_code):\n",
        "    group = df[(df['GeoAreaName'] == country) & (df['Location'] == location) & (df['SeriesCode'] == series_code)]\n",
        "    if group.empty:\n",
        "        return None\n",
        "    group = group.sort_values('TimePeriod')\n",
        "    years = group['TimePeriod'].values\n",
        "    values = group['Value'].values\n",
        "\n",
        "    # Check if already achieved in historical data\n",
        "    achieved_idx = np.where(values >= 100)[0]\n",
        "    if len(achieved_idx) > 0:\n",
        "        return years[achieved_idx[0]]\n",
        "\n",
        "   # Train model\n",
        "    window_size = 3\n",
        "    model = train_lstm(values, window_size)\n",
        "    if model is None:\n",
        "        return None\n",
        "\n",
        "    current_year = years[-1]\n",
        "    current_series = list(values / 100.0)\n",
        "    max_year = 2100\n",
        "    while current_series[-1] < 1.0 and current_year < max_year:\n",
        "        last_window = torch.tensor(current_series[-window_size:], dtype=torch.float32).unsqueeze(-1).unsqueeze(0)  # [batch, seq, feat]\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred_norm = model(last_window).item()\n",
        "        pred_value = pred_norm * 100\n",
        "        if pred_value >= 100:\n",
        "            return current_year + 1\n",
        "        # If not increasing significantly, stop to avoid infinite loop\n",
        "        if pred_norm <= current_series[-1]:\n",
        "            return None\n",
        "        current_series.append(pred_norm)\n",
        "        current_year += 1\n",
        "    return None if current_year >= max_year else current_year + 1"
      ],
      "metadata": {
        "id": "hHHKlILL2nOy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load DataSet"
      ],
      "metadata": {
        "id": "wVefNX2UQfFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    df = pd.read_excel(filename, engine='odf')\n",
        "    df = df[df['Indicator'] == '1.4.1']\n",
        "    df = df[['GeoAreaName', 'Location', 'SeriesCode', 'TimePeriod', 'Value']]\n",
        "    df = df[df['GeoAreaName'] != 'World']\n",
        "    print(\"Data loaded successfully from ODS file.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File '{filename}' not found. Please make sure the file is uploaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the ODS file: {e}\")\n",
        "\n",
        "# Unique countries and locations\n",
        "countries = sorted(df['GeoAreaName'].unique())\n",
        "locations = sorted(df['Location'].unique())\n",
        "series_water = 'SP_ACS_BSRVH2O'\n",
        "series_san = 'SP_ACS_BSRVSAN'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2eyxC3WQeNY",
        "outputId": "c8f5b81f-5667-47a8-f559-ee55290d73a8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully from ODS file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Precompute for faster processing"
      ],
      "metadata": {
        "id": "iA-k5uSbQUbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Precompute predicted years\n",
        "precomp_data = []\n",
        "for country in countries:\n",
        "    for loc in locations:\n",
        "        year_water = predict_year_for_series(country, loc, series_water)\n",
        "        year_san = predict_year_for_series(country, loc, series_san)\n",
        "        if year_water is None or year_san is None:\n",
        "            overall_year = None\n",
        "        else:\n",
        "            overall_year = max(year_water, year_san)\n",
        "        precomp_data.append({\n",
        "            'GeoAreaName': country,\n",
        "            'Location': loc,\n",
        "            'Water_Year': year_water if year_water else 'Never',\n",
        "            'Sanitation_Year': year_san if year_san else 'Never',\n",
        "            'Overall_Year': overall_year if overall_year else 'Never'\n",
        "        })\n",
        "\n",
        "precomp_df = pd.DataFrame(precomp_data)\n"
      ],
      "metadata": {
        "id": "nVh6XrB3QX2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Gradio Interface"
      ],
      "metadata": {
        "id": "ytCbxtxA3MAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_year(country, location, series_code):\n",
        "    year = predict_year_for_series(country, location, series_code)\n",
        "    if year:\n",
        "        return f\"Predicted year to reach 100%: {year}\"\n",
        "    else:\n",
        "        return \"Prediction not possible with available data or already achieved/stagnated.\"\n",
        "\n",
        "# Get unique values for dropdowns\n",
        "countries = df['GeoAreaName'].unique().tolist()\n",
        "locations = df['Location'].unique().tolist()\n",
        "series_codes = df['SeriesCode'].unique().tolist()\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Predict Year to Reach 100%\")\n",
        "    with gr.Row():\n",
        "        country_dropdown = gr.Dropdown(choices=countries, label=\"Select Country\")\n",
        "        location_dropdown = gr.Dropdown(choices=locations, label=\"Select Location\")\n",
        "        series_code_dropdown = gr.Dropdown(choices=series_codes, label=\"Select Series Code\")\n",
        "    predict_button = gr.Button(\"Predict\")\n",
        "    output_text = gr.Textbox(label=\"Prediction Result\")\n",
        "\n",
        "    predict_button.click(fn=predict_year, inputs=[country_dropdown, location_dropdown, series_code_dropdown], outputs=output_text)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "VWgbTQlPKCD-",
        "outputId": "951a0955-c516-4470-9c27-359c7006aab9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://598a019a97ad0c14eb.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://598a019a97ad0c14eb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}